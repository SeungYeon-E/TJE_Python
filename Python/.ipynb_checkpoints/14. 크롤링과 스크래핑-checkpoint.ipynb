{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65f3524",
   "metadata": {},
   "source": [
    "# 데이터 다운로드 하기\n",
    ": 인터넷에서 저장된 파일을 내 PC에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c95aefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 되었습니다!\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import urllib.request\n",
    "\n",
    "# url과 저장경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "urllib.request.urlretrieve(url, savename)\n",
    "print(\"저장 되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f9b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import urllib.request\n",
    "\n",
    "# url과 저장경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test1.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "# 파일로 저장하기\n",
    "with open(savename, \"wb\") as f:\n",
    "    f.write(mem)\n",
    "print(\"저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d43e9",
   "metadata": {},
   "source": [
    "# BeatifulSoup로 Scraping하기\n",
    ": 간단하게 HTML과 XML에서 정보를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a7b073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ham\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ham\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd781933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Header</h1>\n",
      "<p> Line 1을 서술 </p>\n",
      "--------\n",
      "h1= Header\n",
      "h1= Header\n",
      "p1=  Line 1을 서술 \n",
      "p1=  Line 1을 서술 \n"
     ]
    }
   ],
   "source": [
    "# 기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <h1>Header</h1>\n",
    "                <p> Line 1을 서술 </p>\n",
    "            </body>\n",
    "        </html>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# print(soup)\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "print(h1)\n",
    "print(p1)\n",
    "print(\"--------\")\n",
    "\n",
    "# text만 출력\n",
    "print(\"h1=\", h1.string)\n",
    "print(\"h1=\", h1.text)\n",
    "print(\"p1=\", p1.string)\n",
    "print(\"p1=\", p1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683f9d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\"> Header </h1>\n",
      "<p id=\"body\"> Line 1을 서술 </p>\n",
      "title =  Header \n",
      "body =  Line 1을 서술 \n"
     ]
    }
   ],
   "source": [
    "# id로 요소를 추출하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1 id=\"title\"> Header </h1>\n",
    "            <p id=\"body\"> Line 1을 서술 </p>\n",
    "        </body>\n",
    "    </html>\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "title = soup.find(id = 'title')\n",
    "body = soup.find(id = 'body')\n",
    "\n",
    "print(title)\n",
    "print(body)\n",
    "\n",
    "# text만 출력\n",
    "print(\"title =\", title.string)\n",
    "print(\"body =\", body.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "401082f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver >>> http://www.naver.com\n",
      "daum >>> http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 요소 추출하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "            <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# 링크 목록 출력\n",
    "for link in links:\n",
    "#     print(link.string)\n",
    "#     print(link.attrs['href'])\n",
    "    href = link.attrs['href']\n",
    "    text = link.string\n",
    "    print(text, \">>>\", href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840fe7f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "○ (강수) 16일(금) 오후에는 소나기가 내리는 곳이 있겠고, 19일(월) 오후에는 비가 내리겠습니다.\n",
      "○ (기온) 이번 예보기간 아침최저기온은 23~26도, 낮최고기온은 29~34도로 어제(12일, 아침최저기온 20~24도, 낮최고기온 30~33도)와 비슷하거나 조금 높겠습니다.\n",
      "○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.\n",
      "○ (주말전망) 17일(토)~18일(일)은 구름많겠고, 아침 기온은 23~24도, 낮 기온은 29~33도가 되겠습니다\n",
      "\n",
      "* 이번 예보기간 동안 내륙에는 낮최고기온이 33도 내외, 아침최저기온이 25도 내외로 오르는 곳이 많아 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다.\n",
      "* 또한, 북태평양고기압 위치에 따른 강수 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "○ (강수) 16일(금) 오후에는 소나기가 내리는 곳이 있겠고, 19일(월) 오후에는 비가 내리겠습니다. ○ (기온) 이번 예보기간 아침최저기온은 23~26도, 낮최고기온은 29~34도로 어제(12일, 아침최저기온 20~24도, 낮최고기온 30~33도)와 비슷하거나 조금 높겠습니다. ○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다. ○ (주말전망) 17일(토)~18일(일)은 구름많겠고, 아침 기온은 23~24도, 낮 기온은 29~33도가 되겠습니다  * 이번 예보기간 동안 내륙에는 낮최고기온이 33도 내외, 아침최저기온이 25도 내외로 오르는 곳이 많아 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다. * 또한, 북태평양고기압 위치에 따른 강수 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기상청 자료 활용하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "\n",
    "# data 가져오기\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "# print(soup)\n",
    "\n",
    "#원하는 데이터 추출하기\n",
    "title = soup.find('title').string\n",
    "# print(title)\n",
    "wf = soup.find('wf').string\n",
    "# print(wf)\n",
    "# type(wf)\n",
    "\n",
    "# 데이터 정리하기\n",
    "wf_str = str(wf)\n",
    "wf_list = wf_str.split('<br />')\n",
    "\n",
    "for i in wf_list:\n",
    "    print(i)\n",
    "    \n",
    "# 데이터 정리하기\n",
    "list_wfs = list(wf)\n",
    "except_char = ['<', 'b', 'r', '/', '>']\n",
    "result = \"\"\n",
    "\n",
    "for lwf in list_wfs:\n",
    "    if lwf not in except_char:\n",
    "        result += lwf\n",
    "        \n",
    "print('-' * 100)\n",
    "print(result)\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3d734a",
   "metadata": {},
   "source": [
    "# CSS 선택자 사용하기\n",
    "\n",
    "soup.select_one() : css선택자로 요소 하나를 추출.\n",
    "\n",
    "soup.select() : css선택자로 요소 여러개를 리스트로 추출."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2484833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위키 북스\n",
      "유니티 게임 이펙트 입문서\n",
      "스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <div id = \"meigen\">\n",
    "                <h1>위키 북스</h1>\n",
    "                <ul class=\"items\">\n",
    "                    <li>유니티 게임 이펙트 입문서</li>\n",
    "                    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "                    <li>모던 웹사이트 디자인의 정석</li>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 필요한 부분 css로 추출하기\n",
    "# 타이틀 부분 추출하기\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string #  id:# , class:. , >:자손 , \" \":후손\n",
    "print(h1)\n",
    "\n",
    "# 목록 부분 추출하기\n",
    "li_lists = soup.select(\"div#meigen > ul.items > li\")\n",
    "# print(li_lists)\n",
    "for li in li_lists:\n",
    "    print(li.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea1f5482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 금융에서 환율 정보 추출하기\n",
    "# https://finance.naver.com/marketindex/\n",
    "# document.querySelector(\"#exchangeList > li.on > a.head.usd > div > span.value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16214c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd /krw :  1,144.10\n"
     ]
    }
   ],
   "source": [
    "# 미국 환율 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"http://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "price = soup.select_one(\"div.head_info > span.value\").string\n",
    "print(\"usd /krw : \", price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd631ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 : 1,144.10\n",
      "일본 : 1,036.04\n",
      "유렵연합 : 1,357.36\n",
      "중국 : 176.83\n"
     ]
    }
   ],
   "source": [
    "# 미국, 일본, 유럽연합, 중국의 환율\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"http://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "price = soup.select(\"div.head_info > span.value\")\n",
    "# for i in price:\n",
    "#     print(i.string)\n",
    "print(\"미국 :\", price[0].string)\n",
    "print(\"일본 :\", price[1].string)\n",
    "print(\"유렵연합 :\", price[2].string)\n",
    "print(\"중국 :\", price[3].string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a3de403a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘과 바람과 별과 시\n",
      "서시\n",
      "자화상\n",
      "소년\n",
      "눈 오는 지도\n",
      "돌아와 보는 밤\n",
      "병원\n",
      "새로운 길\n",
      "간판 없는 거리\n",
      "태초의 아침\n",
      "또 태초의 아침\n",
      "새벽이 올 때까지\n",
      "무서운 시간\n",
      "십자가\n",
      "바람이 불어\n",
      "슬픈 족속\n",
      "눈감고 간다\n",
      "또 다른 고향\n",
      "길\n",
      "별 헤는 밤\n",
      "흰 그림자\n",
      "사랑스런 추억\n",
      "흐르는 거리\n",
      "쉽게 씌어진 시\n",
      "봄\n",
      "참회록\n",
      "간(肝)\n",
      "위로\n",
      "팔복\n",
      "못자는밤\n",
      "달같이\n",
      "고추밭\n",
      "아우의 인상화\n",
      "사랑의 전당\n",
      "이적\n",
      "비오는 밤\n",
      "산골물\n",
      "유언\n",
      "창\n",
      "바다\n",
      "비로봉\n",
      "산협의 오후\n",
      "명상\n",
      "소낙비\n",
      "한난계\n",
      "풍경\n",
      "달밤\n",
      "장\n",
      "밤\n",
      "황혼이 바다가 되어\n",
      "아침\n",
      "빨래\n",
      "꿈은 깨어지고\n",
      "산림\n",
      "이런날\n",
      "산상\n",
      "양지쪽\n",
      "닭\n",
      "가슴 1\n",
      "가슴 2\n",
      "비둘기\n",
      "황혼\n",
      "남쪽 하늘\n",
      "창공\n",
      "거리에서\n",
      "삶과 죽음\n",
      "초한대\n",
      "산울림\n",
      "해바라기 얼굴\n",
      "귀뚜라미와 나와\n",
      "애기의 새벽\n",
      "햇빛·바람\n",
      "반디불\n",
      "둘 다\n",
      "거짓부리\n",
      "눈\n",
      "참새\n",
      "버선본\n",
      "편지\n",
      "봄\n",
      "무얼 먹구 사나\n",
      "굴뚝\n",
      "햇비\n",
      "빗자루\n",
      "기왓장 내외\n",
      "오줌싸개 지도\n",
      "병아리\n",
      "조개껍질\n",
      "겨울\n",
      "트루게네프의 언덕\n",
      "달을 쏘다\n",
      "별똥 떨어진 데\n",
      "화원에 꽃이 핀다\n",
      "종시\n"
     ]
    }
   ],
   "source": [
    "# 윤동주 시안 작품 가져오기\n",
    "# https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# #mw-content-text > div.mw-parser-output > ul > li > ul > li > a\n",
    "# #mw-content-text > div.mw-parser-output > ul > li > b > a\n",
    "\n",
    "# 데이터 추출하기\n",
    "output = soup.select(\"div.mw-parser-output  ul  li  a\")\n",
    "    \n",
    "for i in output:\n",
    "    if i.string ==\n",
    "    '증보판':\n",
    "        continue\n",
    "    print(i.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4061961",
   "metadata": {},
   "source": [
    "# 다음 영화 연간 순위 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "57dcff47",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 남산의 부장들\n",
      "2 : 다만 악에서 구하소서\n",
      "3 : 반도\n",
      "4 : 히트맨\n",
      "5 : 테넷\n",
      "6 : 백두산\n",
      "7 : #살아있다\n",
      "8 : 강철비2: 정상회담\n",
      "9 : 담보\n",
      "10 : 닥터 두리틀\n",
      "11 : 삼진그룹 영어토익반\n",
      "12 : 정직한 후보\n",
      "13 : 도굴\n",
      "14 : 클로젯\n",
      "15 : 오케이 마담\n",
      "16 : 해치지않아\n",
      "17 : 천문: 하늘에 묻는다\n",
      "18 : 결백\n",
      "19 : 1917\n",
      "20 : 작은 아씨들\n",
      "21 : 미드웨이\n",
      "22 : 시동\n",
      "23 : 지푸라기라도 잡고 싶은 짐승들\n",
      "24 : 미스터 주: 사라진 VIP\n",
      "25 : 인비저블맨\n",
      "26 : 나쁜 녀석들: 포에버\n",
      "27 : 국제수사\n",
      "28 : 침입자\n",
      "29 : 스타워즈: 라이즈 오브 스카이워커\n",
      "30 : 스파이 지니어스 \n",
      "31 : 이웃사촌\n",
      "32 : 온워드: 단 하루의 기적\n",
      "33 : 소리도 없이\n",
      "34 : 버즈 오브 프레이(할리 퀸의 황홀한 해방)\n",
      "35 : 원더 우먼 1984\n",
      "36 : 겨울왕국 2\n",
      "37 : 오! 문희\n",
      "38 : 그린랜드\n",
      "39 : 위대한 쇼맨\n",
      "40 : 런\n",
      "41 : 뮬란\n",
      "42 : 내가 죽던 날\n",
      "43 : 기생충\n",
      "44 : 신비아파트 극장판 하늘도깨비 대 요르문간드\n",
      "45 : 프리즌 이스케이프\n",
      "46 : 검객\n",
      "47 : 조제\n",
      "48 : 사라진 시간\n",
      "49 : 밤쉘: 세상을 바꾼 폭탄선언\n",
      "50 : 알라딘\n"
     ]
    }
   ],
   "source": [
    "# https://movie.daum.net/ranking/boxoffice/yearly\n",
    "# mainContent > div > div.box_boxoffice > ol > li:nth-child(1) > div > div.thumb_cont > strong > a\n",
    "#mainContent > div > div.box_boxoffice > ol > li:nth-child(1) > div > div.thumb_item > div.poster_movie > span.rank_num\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "\n",
    "url1 = \"https://movie.daum.net/ranking/boxoffice/yearly\"\n",
    "res1 = req.urlopen(url1)\n",
    "\n",
    "soup1 = BeautifulSoup(res1, \"html.parser\")\n",
    "\n",
    "ranking = soup1.select(\"div.box_boxoffice > ol > li > div > div.thumb_cont > strong > a\")\n",
    "\n",
    "count = 1\n",
    "for i in ranking:\n",
    "    print(count, ':', i.string)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaccefc",
   "metadata": {},
   "source": [
    "# 다음 IT News 많이 본 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c84de3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://v.daum.net/v/20210713140143407 : 이재명, 긴급 기자회견..\"4차 팬데믹 못막으면 전면봉쇄 불가피\"\n",
      "https://v.daum.net/v/20210713132549158 : BJ철구, 7살딸과 '여캠BJ 새엄마 월드컵'..아동학대 논란\n",
      "https://v.daum.net/v/20210713144711324 : \"일출 보려다 물어뜯길 뻔\"..성산일출봉까지 점령한 야생들개\n",
      "https://v.daum.net/v/20210713124446148 : \"누나가 무슨 부모야\" 친누나 30차례 찔러 살해한 남동생.. 무기징역 구형\n",
      "https://v.daum.net/v/20210713114057051 : \"술집 못가니 호텔 가자\" 60대 유명화가, 20대 성폭행 의혹\n",
      "https://v.daum.net/v/20210713133842565 : 정은경 '靑 방역기획관에 밀렸나' 질문에 \"소신껏 하고 있다\" 일축\n",
      "https://v.daum.net/v/20210713144622300 : \"확진자인데 와서 미안\" 농담에 카페 영업중단..업무방해 무죄\n",
      "https://v.daum.net/v/20210713113441712 : 우리은행 본점서 코로나19 집단감염..\"발설 시 엄벌\"\n",
      "https://v.daum.net/v/20210713113230612 : 홍남기, 전국민 지원금 합의에 \"동의 안한다\"..여당과 충돌\n",
      "https://v.daum.net/v/20210713143600846 : 청와대 앞에 착륙한 미끼 전투기, 힘을 보여주고 싶었다\n",
      "https://v.daum.net/v/20210713141559923?s=eRIGHT_MANY_TOT=R : 옥주현 \"큰아버지 빚 상속받았다..정말 황당\"\n",
      "https://v.daum.net/v/20210713140557574?s=eRIGHT_MANY_TOT=R : 'SKY캐슬' 이유진, 벌써 18살..184cm 넘는 모델 피지컬\n",
      "https://v.daum.net/v/20210713114046042?s=eRIGHT_MANY_TOT=R : '임신 중 별세' 서보라미 출연 '노는 언니', 오늘 결방 \"잠정 연기\"\n",
      "https://v.daum.net/v/20210713133012274?s=eRIGHT_MANY_TOT=R : 이성경 \"미치광이 수준으로 골프에 빠져있다\" 고백\n",
      "https://v.daum.net/v/20210713135922267?s=eRIGHT_MANY_TOT=R : '장윤정♥︎' 도경완 \"우리 하영이 언니 되겠네\" 셋째 '캠핑 베이비' 욕심\n",
      "https://v.daum.net/v/20210713133018277?s=eRIGHT_MANY_TOT=R : 정은지 \"연예인 친구 많은 줄 아는데 하나도 없어, 난 자발적 아웃사이더\"\n",
      "https://v.daum.net/v/20210713101435560?s=eRIGHT_MANY_TOT=R : 도경완 \"하영이 언니 되겠네\" ♥장윤정과 캠핑 베이비 욕심 불끈\n",
      "https://v.daum.net/v/20210713142325250?s=eRIGHT_MANY_TOT=R : 김동현, 격투기 성공 전 연봉 공개 \"서울 호텔 하수구 다 뚫어\"\n",
      "https://v.daum.net/v/20210713112104884?s=eRIGHT_MANY_TOT=R : 김준호 \"이혼한지 얼마 안 돼 위축돼 있었는데 '돌싱포맨' 후 달라졌다\"\n",
      "https://v.daum.net/v/20210713110846230?s=eRIGHT_MANY_TOT=R : '양다리 논란' 하준수♥안가연 '코빅' 하차요구까지..오늘 녹화 강행?\n",
      "https://v.daum.net/v/20210713114515281 : 케인, 결국 폭발..\"당신들은 팬도 아니야\"\n",
      "https://v.daum.net/v/20210713141552918 : 승부조작 대가 받은 윤성환, \"혐의 사실 모두 인정\"\n",
      "https://v.daum.net/v/20210713112703265 : \"내가 PK 차고 싶다고 했잖아!\" 잉글랜드 승부차기 논란 일파만파\n",
      "https://v.daum.net/v/20210713105817524 : 한국 올림픽 금메달 예측 7위..스페인 1위 일본 6위\n",
      "https://v.daum.net/v/20210713130323627 : \"왜 우리는 야구 하는거죠?\" 그날 최형우가 던진 돌발 질문\n",
      "https://v.daum.net/v/20210713105003169 : '무단침입·칼 부림·성폭행 주장'까지..충격과 공포의 웸블리\n",
      "https://v.daum.net/v/20210713112607210 : 해리 케인 \"너 같은 팬 필요없어\"..인종차별 사태 비난\n",
      "https://v.daum.net/v/20210713120048055 : 알론소, 역대 3번째 홈런더비 2연패 달성..연봉보다 상금이 많아\n",
      "https://v.daum.net/v/20210713110449047 : 대구마저 코로나 확진자 발생..선수 1명 확진\n",
      "https://v.daum.net/v/20210713113002424 : 황의조의 보르도, 2부 강등→리그앙으로 구사일생..'결정 뒤집혔다'\n"
     ]
    }
   ],
   "source": [
    "#mAside > div.aside_g.aside_ranking > ul > li.on > div > ol:nth-child(2) > li:nth-child(1) > strong > a\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url1 = \"https://news.daum.net/digital#1\"\n",
    "res1 = req.urlopen(url1)\n",
    "\n",
    "soup1 = BeautifulSoup(res1, \"html.parser\")\n",
    "\n",
    "ranking = soup1.select(\"ol:nth-child(2) > li > strong > a\") \n",
    "\n",
    "for i in ranking:\n",
    "    href = i.attrs['href']\n",
    "    text = i.string\n",
    "    print(href.strip(), \":\", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44224287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d672d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c11b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c50b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077cb41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97733f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6eb2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579fee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11222d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f9e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c99ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff20109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
